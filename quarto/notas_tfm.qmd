---
title: Notas TFM
date: today

format:
    html: default

toc: true
toc-title: Content
code-fold: true
wrap: auto
embed-resources: true
warning: false
error: false
---

# Introduction

# Research question
Can we use vector representation of words to detect the difference between inflection and derivation?
  
# Methodology
This study explores the distinction between inflection and derivation in Polish and Spanish using various word embedding models, both static and contextual.

To achieve this, we implemented several models. For static embeddings, we used Word2Vec and FastText, and for contextual embeddings, we utilized the Multilingual BERT model. The Word2Vec model for Spanish was based on the Spanish Billion Words corpus. For Polish, we used the IPIPAN Word2Vec model (nkjp+wiki-forms-all-300-skipg-ns), which is comparable in quality to SBW. It was trained on the National Corpus of Polish (NKJP) and Wikipedia, includes all parts of speech and word forms, and produces 300-dimensional vectors using the skip-gram algorithm with negative sampling.

In addition, we applied FastText embeddings for both Spanish and Polish to incorporate subword-level information. For contextual representation, we used Multilingual BERT (mBERT) to capture context-sensitive nuances in the morphological structures.

In order to conduct an initial analysis we constructed two separate datasets, using data from UniMorph, one for inflection and another one for derivation.

## Inflection Dataset
For the inflection analysis, we constructed a Pivot/Inflection dataset limited to verb-to-verb (V:V) transformations, since no other category is possible.

* We filtered the data to include the following verb tenses in Spanish:
  * Present Indicative. UniMorph category: V;IND;PRS.
  * Past Imperfect. UniMorph category: V;IND;PST;IPFV
  * Future Indicative. UniMorph category: V;IND;FUT

This resulted in a dataset of 148,051 rows, each consisting of a base form, its inflected variant, and the morphological category. Additional forms such as participles and gerunds are planned for future inclusion.

* In Polish the filtering included:
  * Present. UniMorph category: V;PRS.
  * Past. UniMorph category:V;PST.
  * Future. UniMorph category:V;FUT.

The resulting dataset contains 23,615 rows, structured similarly to the Spanish set with base, inflected form, and category.

<!-- ejemplito de dataset por aqui -->

## Derivation Dataset
For derivational morphology, we also used UniMorph as the primary data source. The selection and preprocessing steps for this dataset are ongoing and will be aligned with the methodology applied to the inflection dataset to ensure consistency in the analysis.

## Tables
The initial results are shown in the following tables. 

Mean similarity between pivot and inflection.  

| INFLECTION | WORD2VEC | FASTTEXT | MULTILINGUAL BERT |
| ---------- | -------- | -------- | ----------------- |
| **SPA**    | 0.539277 | 0.555474 | 0.95              |
| **POL**    | 0.513006 | 0.486620 | 0.91              |
|            |          |          | *POLBERT 0.84*    |
|            |          |          | *HERBERT 0.89*    |

Mean similarity between pivot and derivation.

| DERIVATION | WORD2VEC | FASTTEXT | MULTILINGUAL BERT |
| ---------- | -------- | -------- | ----------------- |
| **SPA**    | 0.504810 | 0.511147 | 0.927623          |
| **POL**    | 0.406683 | 0.544072 | 0.931450          |
|            |          |          | *POLBERT 0.89*    |
|            |          |          | *HERBERT 0.92*    |

# Code and plots
In this section we plot the initial results. Mean similarity of the category (V:V, ADJ:N, ADV:ADJ...) by type (inflection, derivation between the same categories and derivation between different categories), separated by embeddings model and language.

```{r}
#| echo: false
# set working directory
# print(getwd())
# setwd("C:/PythonCode/TFM_GLS/")
# print(getwd())
```

```{r}
#| echo: false
library(ggplot2)
library(dplyr)

# FASTTEXT
# INFLECTION
spa_ft_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_fasttext_inflection_results.csv")
pol_ft_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_fasttext_inflection_results.csv")
# DERIVATION
spa_ft_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_fasttext_derivation_results.csv")
pol_ft_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_fasttext_derivation_results.csv")

# WORD2VEC
# INFLECTION
spa_w2v_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_word2vec_inflection_results.csv")
pol_w2v_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_word2vec_inflection_results.csv")
# DERIVATION
spa_w2v_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_word2vec_derivation_results.csv")
pol_w2v_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_word2vec_derivation_results.csv")

# BERT
# INFLECTION
spa_bert_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_bert_inflection_results.csv")
pol_bert_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_bert_inflection_results.csv")
# DERIVATION
spa_bert_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_bert_derivation_results.csv")
pol_bert_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_bert_derivation_results.csv")

# adding model, language and type (inf or der) to the dataset in order to plot it
spa_ft_inf <- spa_ft_inf %>%
  mutate(model = "FastText", language = "Spanish", type = "Inflection")

pol_ft_inf <- pol_ft_inf %>%
  mutate(model = "FastText", language = "Polish", type = "Inflection")

spa_ft_der <- spa_ft_der %>%
  mutate(model = "FastText", language = "Spanish", type = "Derivation")

pol_ft_der <- pol_ft_der %>%
  mutate(model = "FastText", language = "Polish", type = "Derivation")

spa_w2v_inf <- spa_w2v_inf %>%
  mutate(model = "Word2Vec", language = "Spanish", type = "Inflection")

pol_w2v_inf <- pol_w2v_inf %>%
  mutate(model = "Word2Vec", language = "Polish", type = "Inflection")

spa_w2v_der <- spa_w2v_der %>%
  mutate(model = "Word2Vec", language = "Spanish", type = "Derivation")

pol_w2v_der <- pol_w2v_der %>%
  mutate(model = "Word2Vec", language = "Polish", type = "Derivation")

spa_bert_inf <- spa_bert_inf %>%
  mutate(model = "Multilingual BERT", language = "Spanish", type = "Inflection")

pol_bert_inf <- pol_bert_inf %>%
  mutate(model = "Multilingual BERT", language = "Polish", type = "Inflection")

spa_bert_der <- spa_bert_der %>%
  mutate(model = "Multilingual BERT", language = "Spanish", type = "Derivation")

pol_bert_der <- pol_bert_der %>%
  mutate(model = "Multilingual BERT", language = "Polish", type = "Derivation")

```

```{r}
#| echo: false

# spa_ft_inf %>% filter(!grepl("Ã¡s", inflection) & grepl("PRS;2;SG", category), 0, ifelse(grepl("FORM", category), 0, 1)))
# spa_ft_der %>% filter (category == "ADJ:ADJ")
# sample

# inflection and derivation datasets
all_inflection <- bind_rows(spa_ft_inf, pol_ft_inf, spa_w2v_inf,
 pol_w2v_inf, spa_bert_inf, pol_bert_inf,)

all_inflection_spa <- bind_rows(spa_ft_inf, spa_w2v_inf, spa_bert_inf)

all_derivation <- bind_rows(spa_ft_der, pol_ft_der, spa_w2v_der,
 pol_w2v_der, spa_bert_der, pol_bert_der)

# # add a new column with "same category" or "different cattegory" to the derivations dataset
all_derivation <- all_derivation %>%
  mutate(type = ifelse(
    gsub(":.*", "", category) == gsub(".*:", "", category),
    "Derivation (same category)", "Derivation (different category)"))

# factoring data (type: inflection, derivation same, derivation diff) on both datasets
all_derivation$type <- as.factor(all_derivation$type)
all_inflection$type <- as.factor(all_inflection$type)
all_inflection_spa$category <- as.factor(all_inflection_spa$category)
# join both datasets
all_df <- bind_rows(all_derivation, all_inflection)

# factor model, language and category
all_df$model <- as.factor(all_df$model)
all_df$language <- as.factor(all_df$language)
all_df$category <- as.factor(all_df$category)

# group by category, model and language and get the mean similarity of each, to plot
# i.e.:       category   model    language   type      mean_similarity
#              ADJ:ADJ   FasText  Polish   Inflection   0.6197
all_df_plot <- all_df %>%
  group_by(category, model, language, type) %>%
  summarise(mean_similarity = mean(similarity, na.rm = TRUE), .groups = 'drop')

# plot1 <- ggplot(all_df_plot, aes(x = type, y = mean_similarity, color = type)) +
#   geom_point(size = 5, na.rm = TRUE, alpha = 0.5) +
#   labs(x = "", y = "Mean similarity") +
#   facet_grid(language ~ model) +
#   stat_summary(fun.data = "mean_cl_boot", 
#     color = "black", size = 0.8, 
#     position = position_dodge(0.85)) +
#   theme(
#     plot.margin = margin(t = 10, r = 40, b = 10, l = 60),
#     axis.text.x = element_text(angle = 45, hjust = 1, size = 18),
#     axis.text.y = element_text(size = 16),
#     strip.text = element_text(size = 18)) +
#   scale_color_brewer(palette = "Dark2") +
#   guides(color = "none")
# plot1
```

## Mean similarity between pivot-form in derivation and inflection by model and language
```{r}
#| column: page
#| fig-format: svg
#| out-width: 90%
#| fig-width: 22
#| fig-height: 14
plot1 <- ggplot(all_df_plot, aes(x = type, y = mean_similarity, fill = type)) +
  geom_dotplot(
    binaxis = "y", stackdir = "center", 
    dotsize = 3, binwidth = 0.01, 
    na.rm = TRUE, alpha = 0.8
  ) +
  labs(x = "", y = "Mean similarity") +
  facet_grid(language ~ model) +
  stat_summary(
    fun.data = "mean_cl_boot", 
    color = "black", size = 1, 
    position = position_dodge(0.85)
  ) +
  theme(
    plot.margin = margin(t = 10, r = 20, b = 10, l = 40),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 18),
    axis.text.y = element_text(size = 16),
    strip.text = element_text(size = 18)
  ) +
  scale_fill_brewer(palette = "Dark2") +
  guides(fill = "none")
plot1
```

```{r}
#| echo: false
# all_derivation_plot <- all_derivation %>%
#   group_by(category, model, language, type) %>%
#   summarise(mean_similarity = mean(similarity, na.rm = TRUE), .groups = 'drop')

# plot2 <- ggplot(all_derivation_plot, aes(x = type, y = mean_similarity)) +
#   geom_point(size = 5, na.rm = TRUE, alpha = 0.5) +
#   labs(x = "", y = "Mean similarity") +
#   facet_wrap(~category, scales = "free_x") +
#   scale_x_discrete(drop = TRUE) +
#   stat_summary(fun.data = "mean_cl_boot", color = "black", size = 0.8, position = position_dodge(0.85)) +
#   theme(
#     plot.margin = margin(t = 10, r = 40, b = 10, l = 60),
#     axis.text.x = element_text(angle = 5, hjust = 1, size = 14),
#     strip.text = element_text(size = 18)) +
#   guides(x = "none")
# plot2
```


```{r}
#| column: body
#| echo: false
#| fig-format: svg
#| out-width: 100%
# all_inflection <- bind_rows(spa_ft_inf, pol_ft_inf, spa_w2v_inf, 
# pol_w2v_inf, spa_bert_inf, pol_bert_inf)

# mean_by_category <- all_inflection  %>% 
#   group_by(category, model, language) %>% 
#   summarise(mean_similarity = mean(similarity, na.rm = TRUE), .groups = "drop")

# plot2 <- ggplot(mean_by_category, aes(x = category, y = mean_similarity, color = model)) +
#   geom_point(size = 3, position = position_dodge(width = 0.5), na.rm = TRUE) +
#   labs(x = "Category", y = "Mean similarity", 
#   color = "Embeddings model", shape = "Language") +
#   facet_wrap(~language, scales = "free_x") +
#   scale_x_discrete(drop = TRUE) +
#   theme_light() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
# plot2

```