---
title: Notas TFM
date: today

format:
    html: default

toc: true
toc-title: Content
code-fold: true
wrap: auto
embed-resources: true
warning: false
error: false
---

# **Research question**:
Can we use vector representation of words to detect the difference between inflection and derivation?
  
# **Methodology**  
* Using word embedding models (static vs. contextual) to identify the distinction between inflection and derivation in Polish and Spanish.   
	* Implemented:   
		* Word2Vec (Spanish: Spanish Billion Words, Polish: IPIPAN model [nkjp+wiki-forms-all-300-skipg-ns](https://dsmodels.nlp.ipipan.waw.pl/))  
			* The polish model is similar to SBW it was trained on NKJP and Wikipedia, contains forms, all PoS, vectors have 300 dimensions and it was trained using skipgram and negative sampling. The file was .txt and i converted it to .bin.  
		* FastText (Spanish, Polish)   
		* BERT models:  
			* Multilingual BERT  
* Datasets:  
	* **INFLECTION**: UniMorph **Base/Inflection (2)** **V:V**.  
		* Filtered **Spanish** to create a Base/Inflection dataset of V:V in present, past imperfect and future.  
			* **PRESENT** V;IND;PRS  
			* **PAST IMPERFECT** V;IND;PST;IPFV  
			* **FUTURE** V;IND;FUT  
			* *TO DO: PARTICIPIOS AND GERUNDIOS*  
			* Result: 148051 rows (Base/Inflection/Category)  
		* Filtered **Polish** data:  
			* **PRESENT** V;PRS  
			* **PAST** V;PST  
			* **FUTURE** V;FUT  
			* Result: 23615 rows (Base/Inflection/Category)  
	* **DERIVATION**: UniMorph.  

## Tables
**MEAN SIMILARITY** biplets dataset (pivot+inflection)  

| INFLECTION | WORD2VEC | FASTTEXT | MULTILINGUAL BERT |
| ---------- | -------- | -------- | ----------------- |
| **SPA**    | 0.539277 | 0.555474 | 0.95              |
| **POL**    | 0.513006 | 0.486620 | 0.91              |
|            |          |          | *POLBERT 0.84*    |
|            |          |          | *HERBERT 0.89*    |

**MEAN SIMILARITY** pivot+derivation

| DERIVATION | WORD2VEC | FASTTEXT | MULTILINGUAL BERT |
| ---------- | -------- | -------- | ----------------- |
| **SPA**    | 0.504810 | 0.511147 | 0.927623          |
| **POL**    | 0.406683 | 0.544072 | 0.931450          |
|            |          |          | *POLBERT 0.89*    |
|            |          |          | *HERBERT 0.92*    |

# Code and plots
```{r}
#| echo: false
# set working directory
# print(getwd())
# setwd("C:/PythonCode/TFM_GLS/")
# print(getwd())
```

```{r}
#| echo: false
library(ggplot2)
library(dplyr)

# FASTTEXT
# INFLECTION
spa_ft_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_fasttext_inflection_results.csv")
pol_ft_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_fasttext_inflection_results.csv")
# DERIVATION
spa_ft_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_fasttext_derivation_results.csv")
pol_ft_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_fasttext_derivation_results.csv")

# WORD2VEC
# INFLECTION
spa_w2v_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_word2vec_inflection_results.csv")
pol_w2v_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_word2vec_inflection_results.csv")
# DERIVATION
spa_w2v_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_word2vec_derivation_results.csv")
pol_w2v_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_word2vec_derivation_results.csv")

# BERT
# INFLECTION
spa_bert_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_bert_inflection_results.csv")
pol_bert_inf <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_bert_inflection_results.csv")
# DERIVATION
spa_bert_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/spa/spa_bert_derivation_results.csv")
pol_bert_der <- read.csv("C:/PythonCode/TFM_GLS/py/results/pol/pol_bert_derivation_results.csv")

# adding model, language and type (inf or der) to the dataset in order to plot it
spa_ft_inf <- spa_ft_inf %>%
  mutate(model = "FastText", language = "Spanish", type = "Inflection")

pol_ft_inf <- pol_ft_inf %>%
  mutate(model = "FastText", language = "Polish", type = "Inflection")

spa_ft_der <- spa_ft_der %>%
  mutate(model = "FastText", language = "Spanish", type = "Derivation")

pol_ft_der <- pol_ft_der %>%
  mutate(model = "FastText", language = "Polish", type = "Derivation")

spa_w2v_inf <- spa_w2v_inf %>%
  mutate(model = "Word2Vec", language = "Spanish", type = "Inflection")

pol_w2v_inf <- pol_w2v_inf %>%
  mutate(model = "Word2Vec", language = "Polish", type = "Inflection")

spa_w2v_der <- spa_w2v_der %>%
  mutate(model = "Word2Vec", language = "Spanish", type = "Derivation")

pol_w2v_der <- pol_w2v_der %>%
  mutate(model = "Word2Vec", language = "Polish", type = "Derivation")

spa_bert_inf <- spa_bert_inf %>%
  mutate(model = "Multilingual BERT", language = "Spanish", type = "Inflection")

pol_bert_inf <- pol_bert_inf %>%
  mutate(model = "Multilingual BERT", language = "Polish", type = "Inflection")

spa_bert_der <- spa_bert_der %>%
  mutate(model = "Multilingual BERT", language = "Spanish", type = "Derivation")

pol_bert_der <- pol_bert_der %>%
  mutate(model = "Multilingual BERT", language = "Polish", type = "Derivation")

```

## Mean similarity of pivot-form in derivation and inflection by model and language
```{r}
#| column: page
#| fig-format: svg
#| out-width: 100%
#| fig-width: 12
#| fig-height: 18

# first plot

# inflection and derivation datasets
all_inflection <- bind_rows(spa_ft_inf, pol_ft_inf, spa_w2v_inf,
 pol_w2v_inf, spa_bert_inf, pol_bert_inf,)

all_derivation <- bind_rows(spa_ft_der, pol_ft_der, spa_w2v_der,
 pol_w2v_der, spa_bert_der, pol_bert_der)

# # add a new column with "same category" or "different cattegory" to the derivations dataset
all_derivation <- all_derivation %>%
  mutate(type = ifelse(
    gsub(":.*", "", category) == gsub(".*:", "", category),
    "Derivation (same category)", "Derivation (different category)"))

# factoring data (type: inflection, derivation same, derivation diff) on both datasets
all_derivation$type <- as.factor(all_derivation$type)
all_inflection$type <- as.factor(all_inflection$type)
# join both datasets
all_df <- bind_rows(all_derivation, all_inflection)

# factor model, language and category
all_df$model <- as.factor(all_df$model)
all_df$language <- as.factor(all_df$language)
all_df$category <- as.factor(all_df$category)

# group by category, model and language and get the mean similarity of each, to plot
# i.e.:       category   model    language   type      mean_similarity
#              ADJ:ADJ   FasText  Polish   Inflection   0.6197
all_df_plot <- all_df %>%
  group_by(category, model, language, type) %>%
  summarise(mean_similarity = mean(similarity, na.rm = TRUE), .groups = 'drop')

plot1 <- ggplot(all_df_plot, aes(x = type, y = mean_similarity, color = type)) +
  geom_point(size = 5, na.rm = TRUE, alpha = 0.5) +
  labs(x = "", y = "Mean similarity") +
  facet_grid(model ~ language) +
  stat_summary(fun.data = "mean_cl_boot", 
    color = "black", size = 0.8, 
    position = position_dodge(0.85)) +
  theme(
    plot.margin = margin(t = 10, r = 40, b = 10, l = 60),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 18),
    axis.text.y = element_text(size = 16),
    strip.text = element_text(size = 18)) +
  scale_color_brewer(palette = "Dark2") +
  guides(color = "none")
plot1
```

## Jitter
```{r}
#| column: page
#| fig-format: svg
#| out-width: 100%
#| fig-width: 12
#| fig-height: 18
#| echo: false

plot2 <- ggplot(all_df_plot, aes(x = type, y = mean_similarity, color = type)) +
  geom_jitter(size = 5, na.rm = TRUE, alpha = 0.8) +
  labs(x = "", y = "Mean similarity") +
  facet_grid(model ~ language) +
  stat_summary(fun.data = "mean_cl_boot", color = "black", size = 0.8, position = position_dodge(0.85)) +
  theme(
    plot.margin = margin(t = 10, r = 40, b = 10, l = 60),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 18),
    axis.text.y = element_text(size = 16),
    strip.text = element_text(size = 18)) +
  scale_color_brewer(palette = "Dark2") +
  guides(color = "none")
plot2
```

## Dotplot (ChatGPT transformed the first one into this one, I had some issues)
```{r}
#| column: page
#| fig-format: svg
#| out-width: 100%
#| fig-width: 12
#| fig-height: 18
plot3 <- ggplot(all_df_plot, aes(x = type, y = mean_similarity, fill = type)) +
  geom_dotplot(
    binaxis = "y", stackdir = "center", 
    dotsize = 3, binwidth = 0.01, 
    na.rm = TRUE, alpha = 0.8
  ) +
  labs(x = "", y = "Mean similarity") +
  facet_grid(model ~ language) +
  stat_summary(
    fun.data = "mean_cl_boot", 
    color = "black", size = 1, 
    position = position_dodge(0.85)
  ) +
  theme(
    plot.margin = margin(t = 10, r = 40, b = 10, l = 60),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 18),
    axis.text.y = element_text(size = 16),
    strip.text = element_text(size = 18)
  ) +
  scale_fill_brewer(palette = "Dark2") +
  guides(fill = "none")
plot3
```

```{r}
#| echo: false
# all_derivation_plot <- all_derivation %>%
#   group_by(category, model, language, type) %>%
#   summarise(mean_similarity = mean(similarity, na.rm = TRUE), .groups = 'drop')

# plot2 <- ggplot(all_derivation_plot, aes(x = type, y = mean_similarity)) +
#   geom_point(size = 5, na.rm = TRUE, alpha = 0.5) +
#   labs(x = "", y = "Mean similarity") +
#   facet_wrap(~category, scales = "free_x") +
#   scale_x_discrete(drop = TRUE) +
#   stat_summary(fun.data = "mean_cl_boot", color = "black", size = 0.8, position = position_dodge(0.85)) +
#   theme(
#     plot.margin = margin(t = 10, r = 40, b = 10, l = 60),
#     axis.text.x = element_text(angle = 5, hjust = 1, size = 14),
#     strip.text = element_text(size = 18)) +
#   guides(x = "none")
# plot2
```


```{r}
#| column: body
#| echo: false
#| fig-format: svg
#| out-width: 100%
# all_inflection <- bind_rows(spa_ft_inf, pol_ft_inf, spa_w2v_inf, 
# pol_w2v_inf, spa_bert_inf, pol_bert_inf)

# mean_by_category <- all_inflection  %>% 
#   group_by(category, model, language) %>% 
#   summarise(mean_similarity = mean(similarity, na.rm = TRUE), .groups = "drop")

# plot2 <- ggplot(mean_by_category, aes(x = category, y = mean_similarity, color = model)) +
#   geom_point(size = 3, position = position_dodge(width = 0.5), na.rm = TRUE) +
#   labs(x = "Category", y = "Mean similarity", 
#   color = "Embeddings model", shape = "Language") +
#   facet_wrap(~language, scales = "free_x") +
#   scale_x_discrete(drop = TRUE) +
#   theme_light() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
# plot2

```